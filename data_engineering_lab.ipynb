{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Programming: Data Collection and Pre-Processing Lab\n",
    "\n",
    "**Author:** Emmanuel (Chooks)  \n",
    "**Course:** PROG8245 - Machine Learning Programming  \n",
    "**Date:** January 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "This notebook demonstrates a complete **15-step Data Engineering roadmap** applied to an e-commerce dataset. We will:\n",
    "- Load and explore raw transactional data\n",
    "- Design appropriate data structures using Python classes\n",
    "- Clean, transform, and engineer features\n",
    "- Serialize data in multiple formats\n",
    "- Generate analytical insights\n",
    "\n",
    "**Dataset:** Synthetic e-commerce sales data with 550 transactions, including intentional data quality issues for cleaning exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from collections import namedtuple\n",
    "from typing import Dict, List, Optional, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Hello, Data!\n",
    "\n",
    "We begin by loading the raw e-commerce CSV file and examining its structure. This initial exploration helps us understand the data's shape, column types, and general characteristics before any processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw CSV data\n",
    "raw_df = pd.read_csv('data/ecommerce_sales.csv')\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"Dataset Shape: {raw_df.shape[0]} rows × {raw_df.shape[1]} columns\")\n",
    "print(f\"\\nColumn Names: {list(raw_df.columns)}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"First 3 Rows:\")\n",
    "print(\"=\"*60)\n",
    "raw_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at data types and non-null counts\n",
    "print(\"Data Types and Non-Null Counts:\")\n",
    "print(\"-\" * 40)\n",
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Pick the Right Container\n",
    "\n",
    "### Design Decision: Data Structure Selection\n",
    "\n",
    "For this e-commerce data engineering task, I need to choose between several Python data structures:\n",
    "\n",
    "| Structure | Pros | Cons | Best Use Case |\n",
    "|-----------|------|------|---------------|\n",
    "| `dict` | Flexible, mutable, key-value access | No attribute access, less structured | Quick lookups, aggregations |\n",
    "| `namedtuple` | Immutable, memory-efficient, attribute access | Cannot modify after creation | Read-only records |\n",
    "| `set` | Fast membership testing, unique values only | Unordered, no duplicates | Deduplication, unique counts |\n",
    "| `class` | Full control, methods, validation | More code to write | Complex objects with behavior |\n",
    "\n",
    "### My Choice: **Custom Class + Dictionary for Aggregation**\n",
    "\n",
    "I'll implement a `SalesRecord` class because:\n",
    "1. **Encapsulation**: I can bundle data with methods like `clean()` and `total()`\n",
    "2. **Validation**: The class can validate data during instantiation\n",
    "3. **Readability**: Attribute access (`record.price`) is cleaner than dict keys (`record['price']`)\n",
    "4. **Extensibility**: Easy to add new methods as requirements evolve\n",
    "\n",
    "I'll also use `set` for tracking unique values (cities, products) and `dict` for aggregations (revenue by city)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Implement Functions and Data Structure\n",
    "\n",
    "Here we implement a `SalesRecord` class that encapsulates individual sales transactions with built-in cleaning and calculation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesRecord:\n",
    "    \"\"\"\n",
    "    Represents a single e-commerce sales transaction.\n",
    "    \n",
    "    Attributes:\n",
    "        order_id: Unique identifier for the order\n",
    "        order_date: Date of the transaction\n",
    "        customer_id: Customer identifier\n",
    "        product: Name of the product purchased\n",
    "        category: Product category\n",
    "        price: Unit price of the product\n",
    "        quantity: Number of units purchased\n",
    "        coupon_code: Applied discount code (if any)\n",
    "        shipping_city: Destination city for shipping\n",
    "        is_cleaned: Flag indicating if record has been cleaned\n",
    "    \"\"\"\n",
    "    \n",
    "    # Coupon discount mapping (class-level constant)\n",
    "    COUPON_DISCOUNTS = {\n",
    "        'SAVE10': 0.10, 'SAVE15': 0.15, 'SAVE20': 0.20, 'SAVE25': 0.25,\n",
    "        'WELCOME5': 0.05, 'FLASH30': 0.30, 'VIP40': 0.40, 'HOLIDAY15': 0.15,\n",
    "        'SUMMER10': 0.10, 'WINTER20': 0.20, 'FREESHIP': 0.00, 'LOYALTY25': 0.25\n",
    "    }\n",
    "    \n",
    "    def __init__(self, order_id: str, order_date: str, customer_id: str,\n",
    "                 product: str, category: str, price: float, quantity: int,\n",
    "                 coupon_code: Optional[str], shipping_city: str):\n",
    "        \"\"\"Initialize a SalesRecord with transaction data.\"\"\"\n",
    "        self.order_id = order_id\n",
    "        self.order_date = order_date\n",
    "        self.customer_id = customer_id\n",
    "        self.product = product\n",
    "        self.category = category\n",
    "        self.price = price\n",
    "        self.quantity = quantity\n",
    "        self.coupon_code = coupon_code\n",
    "        self.shipping_city = shipping_city\n",
    "        self.is_cleaned = False\n",
    "        \n",
    "        # Track data quality issues found\n",
    "        self.quality_issues = []\n",
    "    \n",
    "    def clean(self) -> 'SalesRecord':\n",
    "        \"\"\"\n",
    "        Clean and standardize the record data.\n",
    "        \n",
    "        Cleaning operations:\n",
    "        - Strip whitespace from string fields\n",
    "        - Standardize city names to title case\n",
    "        - Handle negative prices (convert to absolute value)\n",
    "        - Ensure quantity is at least 1\n",
    "        - Validate and standardize coupon codes\n",
    "        \n",
    "        Returns:\n",
    "            self: The cleaned SalesRecord instance (for method chaining)\n",
    "        \"\"\"\n",
    "        # Clean shipping city: strip whitespace and title case\n",
    "        if self.shipping_city:\n",
    "            original_city = self.shipping_city\n",
    "            self.shipping_city = self.shipping_city.strip().title()\n",
    "            if original_city != self.shipping_city:\n",
    "                self.quality_issues.append(f\"City standardized: '{original_city}' -> '{self.shipping_city}'\")\n",
    "        \n",
    "        # Handle negative prices\n",
    "        if self.price is not None and self.price < 0:\n",
    "            self.quality_issues.append(f\"Negative price corrected: {self.price} -> {abs(self.price)}\")\n",
    "            self.price = abs(self.price)\n",
    "        \n",
    "        # Ensure quantity is at least 1\n",
    "        if self.quantity is not None and self.quantity < 1:\n",
    "            self.quality_issues.append(f\"Invalid quantity corrected: {self.quantity} -> 1\")\n",
    "            self.quantity = 1\n",
    "        \n",
    "        # Standardize coupon code to uppercase\n",
    "        if self.coupon_code and isinstance(self.coupon_code, str):\n",
    "            self.coupon_code = self.coupon_code.strip().upper()\n",
    "        \n",
    "        # Clean customer_id\n",
    "        if self.customer_id:\n",
    "            self.customer_id = str(self.customer_id).strip()\n",
    "        \n",
    "        self.is_cleaned = True\n",
    "        return self\n",
    "    \n",
    "    def total(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the total amount for this transaction.\n",
    "        \n",
    "        Applies coupon discount if a valid coupon code is present.\n",
    "        \n",
    "        Returns:\n",
    "            float: Total transaction amount after discount\n",
    "        \"\"\"\n",
    "        if self.price is None or self.quantity is None:\n",
    "            return 0.0\n",
    "        \n",
    "        subtotal = self.price * self.quantity\n",
    "        \n",
    "        # Apply discount if coupon exists\n",
    "        discount = self.get_discount_rate()\n",
    "        return round(subtotal * (1 - discount), 2)\n",
    "    \n",
    "    def get_discount_rate(self) -> float:\n",
    "        \"\"\"\n",
    "        Get the discount rate for the applied coupon.\n",
    "        \n",
    "        Returns:\n",
    "            float: Discount rate (0.0 to 1.0), or 0.0 if no valid coupon\n",
    "        \"\"\"\n",
    "        if self.coupon_code and self.coupon_code in self.COUPON_DISCOUNTS:\n",
    "            return self.COUPON_DISCOUNTS[self.coupon_code]\n",
    "        return 0.0\n",
    "    \n",
    "    def is_valid(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the record has all required fields populated.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if record has no critical missing values\n",
    "        \"\"\"\n",
    "        required_fields = [\n",
    "            self.order_id,\n",
    "            self.order_date,\n",
    "            self.product,\n",
    "            self.price is not None,\n",
    "            self.quantity is not None\n",
    "        ]\n",
    "        return all(required_fields)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert the record to a dictionary for serialization.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary representation of the record\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'order_id': self.order_id,\n",
    "            'order_date': self.order_date,\n",
    "            'customer_id': self.customer_id,\n",
    "            'product': self.product,\n",
    "            'category': self.category,\n",
    "            'price': self.price,\n",
    "            'quantity': self.quantity,\n",
    "            'coupon_code': self.coupon_code,\n",
    "            'shipping_city': self.shipping_city,\n",
    "            'total_amount': self.total(),\n",
    "            'is_cleaned': self.is_cleaned\n",
    "        }\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"String representation for debugging.\"\"\"\n",
    "        return f\"SalesRecord({self.order_id}, {self.product}, ${self.price}, qty={self.quantity})\"\n",
    "\n",
    "\n",
    "# Demonstrate the class with a sample record\n",
    "sample = SalesRecord(\n",
    "    order_id=\"ORD000001\",\n",
    "    order_date=\"2024-06-15\",\n",
    "    customer_id=\"CUST00042\",\n",
    "    product=\"Wireless Earbuds\",\n",
    "    category=\"Electronics\",\n",
    "    price=79.99,\n",
    "    quantity=2,\n",
    "    coupon_code=\"SAVE10\",\n",
    "    shipping_city=\"  new york  \"  # Intentionally dirty\n",
    ")\n",
    "\n",
    "print(\"Before cleaning:\")\n",
    "print(f\"  City: '{sample.shipping_city}'\")\n",
    "print(f\"  Total: ${sample.total()}\")\n",
    "\n",
    "sample.clean()\n",
    "\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(f\"  City: '{sample.shipping_city}'\")\n",
    "print(f\"  Total: ${sample.total()} (after 10% discount)\")\n",
    "print(f\"  Is Valid: {sample.is_valid()}\")\n",
    "print(f\"  Quality Issues Found: {sample.quality_issues}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Bulk Loaded\n",
    "\n",
    "Now we convert the entire DataFrame into a collection of `SalesRecord` objects. This demonstrates mapping data from pandas DataFrames to custom data structures, enabling us to leverage our class methods for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_records_from_dataframe(df: pd.DataFrame) -> List[SalesRecord]:\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to a list of SalesRecord objects.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with e-commerce transaction data\n",
    "        \n",
    "    Returns:\n",
    "        List of SalesRecord objects\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Handle NaN values appropriately\n",
    "        record = SalesRecord(\n",
    "            order_id=row['order_id'],\n",
    "            order_date=row['order_date'] if pd.notna(row['order_date']) else None,\n",
    "            customer_id=row['customer_id'] if pd.notna(row['customer_id']) else None,\n",
    "            product=row['product'] if pd.notna(row['product']) else None,\n",
    "            category=row['category'] if pd.notna(row['category']) else None,\n",
    "            price=float(row['price']) if pd.notna(row['price']) else None,\n",
    "            quantity=int(row['quantity']) if pd.notna(row['quantity']) else None,\n",
    "            coupon_code=row['coupon_code'] if pd.notna(row['coupon_code']) else None,\n",
    "            shipping_city=row['shipping_city'] if pd.notna(row['shipping_city']) else None\n",
    "        )\n",
    "        records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "\n",
    "# Load all records from the raw DataFrame\n",
    "all_records = load_records_from_dataframe(raw_df)\n",
    "\n",
    "print(f\"Loaded {len(all_records)} SalesRecord objects\")\n",
    "print(f\"\\nFirst 3 records:\")\n",
    "for i, rec in enumerate(all_records[:3], 1):\n",
    "    print(f\"  {i}. {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also create a dictionary mapping order_id to records for O(1) lookups\n",
    "records_dict: Dict[str, SalesRecord] = {rec.order_id: rec for rec in all_records}\n",
    "\n",
    "print(f\"Created dictionary with {len(records_dict)} entries\")\n",
    "print(f\"\\nExample lookup: records_dict['ORD000005'] = {records_dict['ORD000005']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Quick Profiling\n",
    "\n",
    "Before diving into cleaning, we need to understand our data's statistical properties. This profiling step reveals the range of values, identifies outliers, and helps us understand what \"normal\" looks like for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_numeric_field(records: List[SalesRecord], field: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate min, mean, max, and std for a numeric field across all records.\n",
    "    \n",
    "    Args:\n",
    "        records: List of SalesRecord objects\n",
    "        field: Name of the numeric attribute to profile\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistical measures\n",
    "    \"\"\"\n",
    "    values = [getattr(rec, field) for rec in records if getattr(rec, field) is not None]\n",
    "    \n",
    "    if not values:\n",
    "        return {'min': None, 'mean': None, 'max': None, 'std': None, 'count': 0}\n",
    "    \n",
    "    return {\n",
    "        'min': round(min(values), 2),\n",
    "        'mean': round(sum(values) / len(values), 2),\n",
    "        'max': round(max(values), 2),\n",
    "        'std': round(np.std(values), 2),\n",
    "        'count': len(values)\n",
    "    }\n",
    "\n",
    "\n",
    "# Profile price field\n",
    "price_stats = profile_numeric_field(all_records, 'price')\n",
    "print(\"PRICE Statistics:\")\n",
    "print(f\"  Min:    ${price_stats['min']}\")\n",
    "print(f\"  Mean:   ${price_stats['mean']}\")\n",
    "print(f\"  Max:    ${price_stats['max']}\")\n",
    "print(f\"  Std:    ${price_stats['std']}\")\n",
    "print(f\"  Count:  {price_stats['count']} non-null values\")\n",
    "\n",
    "# Profile quantity field\n",
    "qty_stats = profile_numeric_field(all_records, 'quantity')\n",
    "print(\"\\nQUANTITY Statistics:\")\n",
    "print(f\"  Min:    {qty_stats['min']}\")\n",
    "print(f\"  Mean:   {qty_stats['mean']}\")\n",
    "print(f\"  Max:    {qty_stats['max']}\")\n",
    "print(f\"  Count:  {qty_stats['count']} non-null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SET for unique value counts - perfect use case for sets!\n",
    "unique_cities: set = {rec.shipping_city for rec in all_records if rec.shipping_city}\n",
    "unique_products: set = {rec.product for rec in all_records if rec.product}\n",
    "unique_categories: set = {rec.category for rec in all_records if rec.category}\n",
    "unique_coupons: set = {rec.coupon_code for rec in all_records if rec.coupon_code}\n",
    "\n",
    "print(\"UNIQUE VALUE COUNTS (using sets):\")\n",
    "print(f\"  Unique Cities:     {len(unique_cities)}\")\n",
    "print(f\"  Unique Products:   {len(unique_products)}\")\n",
    "print(f\"  Unique Categories: {len(unique_categories)}\")\n",
    "print(f\"  Unique Coupons:    {len(unique_coupons)}\")\n",
    "\n",
    "# Show the unique categories\n",
    "print(f\"\\nCategories: {sorted(unique_categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at city distribution - some might have inconsistent casing\n",
    "print(\"City values (first 20 unique):\")\n",
    "for city in sorted(unique_cities)[:20]:\n",
    "    print(f\"  - '{city}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Spot the Grime\n",
    "\n",
    "Data quality issues are inevitable in real-world datasets. In this step, we systematically identify \"dirty\" data that needs cleaning. We look for:\n",
    "\n",
    "1. **Missing values** - NULL or empty fields\n",
    "2. **Invalid values** - Negative prices, zero quantities\n",
    "3. **Inconsistent formatting** - Different cases, extra whitespace\n",
    "4. **Data type issues** - Wrong types or malformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_data_quality_issues(records: List[SalesRecord]) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Scan records and categorize all data quality issues found.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping issue type to list of affected order_ids\n",
    "    \"\"\"\n",
    "    issues = {\n",
    "        'missing_date': [],\n",
    "        'missing_customer_id': [],\n",
    "        'missing_product': [],\n",
    "        'missing_price': [],\n",
    "        'negative_price': [],\n",
    "        'zero_quantity': [],\n",
    "        'inconsistent_city_case': [],\n",
    "        'whitespace_in_city': []\n",
    "    }\n",
    "    \n",
    "    for rec in records:\n",
    "        # Check for missing values\n",
    "        if not rec.order_date:\n",
    "            issues['missing_date'].append(rec.order_id)\n",
    "        if not rec.customer_id or rec.customer_id == '':\n",
    "            issues['missing_customer_id'].append(rec.order_id)\n",
    "        if not rec.product:\n",
    "            issues['missing_product'].append(rec.order_id)\n",
    "        if rec.price is None:\n",
    "            issues['missing_price'].append(rec.order_id)\n",
    "            \n",
    "        # Check for invalid values\n",
    "        if rec.price is not None and rec.price < 0:\n",
    "            issues['negative_price'].append(rec.order_id)\n",
    "        if rec.quantity is not None and rec.quantity == 0:\n",
    "            issues['zero_quantity'].append(rec.order_id)\n",
    "            \n",
    "        # Check for formatting issues\n",
    "        if rec.shipping_city:\n",
    "            if rec.shipping_city != rec.shipping_city.title():\n",
    "                issues['inconsistent_city_case'].append(rec.order_id)\n",
    "            if rec.shipping_city != rec.shipping_city.strip():\n",
    "                issues['whitespace_in_city'].append(rec.order_id)\n",
    "    \n",
    "    return issues\n",
    "\n",
    "\n",
    "# Find all data quality issues\n",
    "quality_issues = identify_data_quality_issues(all_records)\n",
    "\n",
    "print(\"DATA QUALITY ISSUES FOUND:\")\n",
    "print(\"=\" * 50)\n",
    "total_issues = 0\n",
    "for issue_type, affected_orders in quality_issues.items():\n",
    "    count = len(affected_orders)\n",
    "    total_issues += count\n",
    "    if count > 0:\n",
    "        print(f\"\\n{issue_type.upper().replace('_', ' ')}:\")\n",
    "        print(f\"  Count: {count} records\")\n",
    "        print(f\"  Examples: {affected_orders[:3]}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TOTAL ISSUES: {total_issues} across {len(all_records)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine some specific dirty records\n",
    "print(\"Examples of dirty records:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Negative price example\n",
    "if quality_issues['negative_price']:\n",
    "    order_id = quality_issues['negative_price'][0]\n",
    "    rec = records_dict[order_id]\n",
    "    print(f\"\\n1. NEGATIVE PRICE ({order_id}):\")\n",
    "    print(f\"   Product: {rec.product}\")\n",
    "    print(f\"   Price: ${rec.price} ← Invalid!\")\n",
    "\n",
    "# Missing product example\n",
    "if quality_issues['missing_product']:\n",
    "    order_id = quality_issues['missing_product'][0]\n",
    "    rec = records_dict[order_id]\n",
    "    print(f\"\\n2. MISSING PRODUCT ({order_id}):\")\n",
    "    print(f\"   Product: {rec.product} ← NULL\")\n",
    "    print(f\"   Category: {rec.category}\")\n",
    "\n",
    "# Inconsistent city casing example\n",
    "if quality_issues['inconsistent_city_case']:\n",
    "    order_id = quality_issues['inconsistent_city_case'][0]\n",
    "    rec = records_dict[order_id]\n",
    "    print(f\"\\n3. INCONSISTENT CITY CASE ({order_id}):\")\n",
    "    print(f\"   City: '{rec.shipping_city}' ← Needs standardization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Cleaning Rules\n",
    "\n",
    "Now we apply our cleaning logic to fix the identified issues. We'll use the `clean()` method we implemented in our `SalesRecord` class and track before/after metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture \"before\" state\n",
    "before_issues = identify_data_quality_issues(all_records)\n",
    "before_counts = {k: len(v) for k, v in before_issues.items()}\n",
    "\n",
    "print(\"BEFORE CLEANING:\")\n",
    "print(\"-\" * 40)\n",
    "for issue_type, count in before_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {issue_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all records\n",
    "all_quality_issues_found = []\n",
    "\n",
    "for record in all_records:\n",
    "    record.clean()  # Apply cleaning method\n",
    "    if record.quality_issues:\n",
    "        all_quality_issues_found.extend(record.quality_issues)\n",
    "\n",
    "print(f\"Cleaning applied to {len(all_records)} records\")\n",
    "print(f\"\\nQuality fixes applied ({len(all_quality_issues_found)} total):\")\n",
    "for issue in all_quality_issues_found[:10]:  # Show first 10\n",
    "    print(f\"  • {issue}\")\n",
    "if len(all_quality_issues_found) > 10:\n",
    "    print(f\"  ... and {len(all_quality_issues_found) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture \"after\" state\n",
    "after_issues = identify_data_quality_issues(all_records)\n",
    "after_counts = {k: len(v) for k, v in after_issues.items()}\n",
    "\n",
    "print(\"\\nAFTER CLEANING:\")\n",
    "print(\"-\" * 40)\n",
    "for issue_type, count in after_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {issue_type}: {count}\")\n",
    "\n",
    "# Show before/after comparison\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"BEFORE vs AFTER COMPARISON:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Issue Type':<30} {'Before':>8} {'After':>8} {'Fixed':>8}\")\n",
    "print(\"-\" * 54)\n",
    "for issue_type in before_counts:\n",
    "    before = before_counts[issue_type]\n",
    "    after = after_counts[issue_type]\n",
    "    fixed = before - after\n",
    "    if before > 0 or after > 0:\n",
    "        print(f\"{issue_type:<30} {before:>8} {after:>8} {fixed:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Transformations\n",
    "\n",
    "Transformations convert raw data into more useful formats. Here we'll:\n",
    "1. Parse `coupon_code` into a numeric `discount_percent` field\n",
    "2. Calculate `total_amount` with discounts applied\n",
    "3. Extract date components (year, month, day of week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_record_to_enriched_dict(record: SalesRecord) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Transform a SalesRecord into an enriched dictionary with computed fields.\n",
    "    \n",
    "    New fields added:\n",
    "    - discount_percent: Numeric discount from coupon code\n",
    "    - discount_amount: Dollar amount saved\n",
    "    - total_amount: Final amount after discount\n",
    "    - year, month, day_of_week: Date components\n",
    "    \"\"\"\n",
    "    base_dict = record.to_dict()\n",
    "    \n",
    "    # Parse coupon_code to numeric discount\n",
    "    discount_rate = record.get_discount_rate()\n",
    "    base_dict['discount_percent'] = int(discount_rate * 100)\n",
    "    \n",
    "    # Calculate discount amount and total\n",
    "    subtotal = (record.price or 0) * (record.quantity or 0)\n",
    "    base_dict['subtotal'] = round(subtotal, 2)\n",
    "    base_dict['discount_amount'] = round(subtotal * discount_rate, 2)\n",
    "    \n",
    "    # Extract date components\n",
    "    if record.order_date:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(record.order_date, '%Y-%m-%d')\n",
    "            base_dict['year'] = date_obj.year\n",
    "            base_dict['month'] = date_obj.month\n",
    "            base_dict['month_name'] = date_obj.strftime('%B')\n",
    "            base_dict['day_of_week'] = date_obj.strftime('%A')\n",
    "            base_dict['quarter'] = (date_obj.month - 1) // 3 + 1\n",
    "        except ValueError:\n",
    "            base_dict['year'] = None\n",
    "            base_dict['month'] = None\n",
    "            base_dict['month_name'] = None\n",
    "            base_dict['day_of_week'] = None\n",
    "            base_dict['quarter'] = None\n",
    "    else:\n",
    "        base_dict['year'] = None\n",
    "        base_dict['month'] = None\n",
    "        base_dict['month_name'] = None\n",
    "        base_dict['day_of_week'] = None\n",
    "        base_dict['quarter'] = None\n",
    "    \n",
    "    return base_dict\n",
    "\n",
    "\n",
    "# Transform all records\n",
    "transformed_data = [transform_record_to_enriched_dict(rec) for rec in all_records]\n",
    "\n",
    "# Show example transformations\n",
    "print(\"TRANSFORMATION EXAMPLES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find a record with a coupon\n",
    "coupon_examples = [t for t in transformed_data if t['coupon_code']]\n",
    "if coupon_examples:\n",
    "    ex = coupon_examples[0]\n",
    "    print(f\"\\nRecord with Coupon ({ex['order_id']}):\")\n",
    "    print(f\"  Coupon Code:      {ex['coupon_code']}\")\n",
    "    print(f\"  Discount Percent: {ex['discount_percent']}%\")\n",
    "    print(f\"  Subtotal:         ${ex['subtotal']}\")\n",
    "    print(f\"  Discount Amount:  -${ex['discount_amount']}\")\n",
    "    print(f\"  Total Amount:     ${ex['total_amount']}\")\n",
    "    print(f\"  Date Components:  {ex['year']}/{ex['month']} ({ex['day_of_week']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier viewing\n",
    "transformed_df = pd.DataFrame(transformed_data)\n",
    "\n",
    "print(\"\\nTransformed DataFrame - New Columns:\")\n",
    "new_cols = ['order_id', 'coupon_code', 'discount_percent', 'subtotal', \n",
    "            'discount_amount', 'total_amount', 'year', 'month', 'day_of_week']\n",
    "transformed_df[new_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Feature Engineering\n",
    "\n",
    "Feature engineering creates new variables that can improve analytical insights. We'll add:\n",
    "1. `days_since_purchase` - Recency measure from today's date\n",
    "2. `is_high_value` - Flag for transactions above average\n",
    "3. `customer_segment` - Categorization based on spending patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference date for calculating days_since_purchase\n",
    "REFERENCE_DATE = datetime(2025, 1, 29)  # Today's date\n",
    "\n",
    "def add_engineered_features(data: List[Dict], reference_date: datetime) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Add engineered features to transformed records.\n",
    "    \n",
    "    New features:\n",
    "    - days_since_purchase: Days between order and reference date\n",
    "    - is_high_value: True if total_amount > median\n",
    "    - order_size_category: 'Small', 'Medium', 'Large' based on quantity\n",
    "    - revenue_category: 'Low', 'Medium', 'High' based on total_amount\n",
    "    \"\"\"\n",
    "    # First pass: calculate statistics needed for features\n",
    "    totals = [d['total_amount'] for d in data if d['total_amount']]\n",
    "    median_total = np.median(totals)\n",
    "    percentile_33 = np.percentile(totals, 33)\n",
    "    percentile_67 = np.percentile(totals, 67)\n",
    "    \n",
    "    print(f\"Total Amount Statistics:\")\n",
    "    print(f\"  Median: ${median_total:.2f}\")\n",
    "    print(f\"  33rd percentile: ${percentile_33:.2f}\")\n",
    "    print(f\"  67th percentile: ${percentile_67:.2f}\")\n",
    "    \n",
    "    # Second pass: add features\n",
    "    for record in data:\n",
    "        # Days since purchase\n",
    "        if record['order_date']:\n",
    "            try:\n",
    "                order_date = datetime.strptime(record['order_date'], '%Y-%m-%d')\n",
    "                record['days_since_purchase'] = (reference_date - order_date).days\n",
    "            except ValueError:\n",
    "                record['days_since_purchase'] = None\n",
    "        else:\n",
    "            record['days_since_purchase'] = None\n",
    "        \n",
    "        # High value flag\n",
    "        record['is_high_value'] = record['total_amount'] > median_total if record['total_amount'] else False\n",
    "        \n",
    "        # Order size category\n",
    "        qty = record.get('quantity', 0) or 0\n",
    "        if qty <= 1:\n",
    "            record['order_size_category'] = 'Small'\n",
    "        elif qty <= 3:\n",
    "            record['order_size_category'] = 'Medium'\n",
    "        else:\n",
    "            record['order_size_category'] = 'Large'\n",
    "        \n",
    "        # Revenue category\n",
    "        total = record['total_amount'] or 0\n",
    "        if total <= percentile_33:\n",
    "            record['revenue_category'] = 'Low'\n",
    "        elif total <= percentile_67:\n",
    "            record['revenue_category'] = 'Medium'\n",
    "        else:\n",
    "            record['revenue_category'] = 'High'\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Add engineered features\n",
    "enriched_data = add_engineered_features(transformed_data, REFERENCE_DATE)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame and show new features\n",
    "enriched_df = pd.DataFrame(enriched_data)\n",
    "\n",
    "print(\"\\nNew Engineered Features:\")\n",
    "feature_cols = ['order_id', 'order_date', 'total_amount', 'days_since_purchase', \n",
    "                'is_high_value', 'order_size_category', 'revenue_category']\n",
    "enriched_df[feature_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distribution summary\n",
    "print(\"\\nFeature Distribution Summary:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nOrder Size Category:\")\n",
    "print(enriched_df['order_size_category'].value_counts())\n",
    "\n",
    "print(\"\\nRevenue Category:\")\n",
    "print(enriched_df['revenue_category'].value_counts())\n",
    "\n",
    "print(\"\\nHigh Value Transactions:\")\n",
    "print(enriched_df['is_high_value'].value_counts())\n",
    "\n",
    "print(f\"\\nDays Since Purchase Range: {enriched_df['days_since_purchase'].min()} to {enriched_df['days_since_purchase'].max()} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Mini-Aggregation\n",
    "\n",
    "Now we aggregate our cleaned and enriched data to extract business insights. We'll calculate revenue by shipping city using both dictionary-based aggregation and pandas `groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_revenue_by_city(records: List[Dict]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Aggregate revenue metrics by shipping city using dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping city to {total_revenue, order_count, avg_order_value}\n",
    "    \"\"\"\n",
    "    city_stats = {}\n",
    "    \n",
    "    for record in records:\n",
    "        city = record.get('shipping_city')\n",
    "        if not city:\n",
    "            continue\n",
    "            \n",
    "        if city not in city_stats:\n",
    "            city_stats[city] = {\n",
    "                'total_revenue': 0.0,\n",
    "                'order_count': 0,\n",
    "                'total_quantity': 0\n",
    "            }\n",
    "        \n",
    "        total = record.get('total_amount') or 0\n",
    "        qty = record.get('quantity') or 0\n",
    "        \n",
    "        city_stats[city]['total_revenue'] += total\n",
    "        city_stats[city]['order_count'] += 1\n",
    "        city_stats[city]['total_quantity'] += qty\n",
    "    \n",
    "    # Calculate averages\n",
    "    for city in city_stats:\n",
    "        stats = city_stats[city]\n",
    "        stats['total_revenue'] = round(stats['total_revenue'], 2)\n",
    "        stats['avg_order_value'] = round(stats['total_revenue'] / stats['order_count'], 2)\n",
    "        stats['avg_quantity'] = round(stats['total_quantity'] / stats['order_count'], 2)\n",
    "    \n",
    "    return city_stats\n",
    "\n",
    "\n",
    "# Aggregate using dictionary method\n",
    "city_revenue = aggregate_revenue_by_city(enriched_data)\n",
    "\n",
    "# Sort by total revenue (descending)\n",
    "sorted_cities = sorted(city_revenue.items(), key=lambda x: x[1]['total_revenue'], reverse=True)\n",
    "\n",
    "print(\"REVENUE BY SHIPPING CITY (Dictionary Method):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'City':<20} {'Revenue':>12} {'Orders':>8} {'Avg Order':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for city, stats in sorted_cities[:15]:  # Top 15\n",
    "    print(f\"{city:<20} ${stats['total_revenue']:>10,.2f} {stats['order_count']:>8} ${stats['avg_order_value']:>10,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Using pandas groupby\n",
    "print(\"\\nREVENUE BY CITY (pandas groupby):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "city_summary = enriched_df.groupby('shipping_city').agg({\n",
    "    'total_amount': ['sum', 'mean', 'count'],\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "city_summary.columns = ['Total Revenue', 'Avg Order', 'Order Count', 'Total Qty']\n",
    "city_summary = city_summary.sort_values('Total Revenue', ascending=False)\n",
    "city_summary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional aggregation: Revenue by Category\n",
    "print(\"\\nREVENUE BY PRODUCT CATEGORY:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "category_summary = enriched_df.groupby('category').agg({\n",
    "    'total_amount': ['sum', 'mean'],\n",
    "    'order_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "category_summary.columns = ['Total Revenue', 'Avg Order', 'Order Count']\n",
    "category_summary = category_summary.sort_values('Total Revenue', ascending=False)\n",
    "category_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation: Monthly Trends\n",
    "print(\"\\nMONTHLY REVENUE TREND:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "monthly = enriched_df.groupby(['year', 'month', 'month_name']).agg({\n",
    "    'total_amount': 'sum',\n",
    "    'order_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "monthly.columns = ['Revenue', 'Orders']\n",
    "monthly = monthly.reset_index()\n",
    "monthly = monthly.sort_values(['year', 'month'])\n",
    "monthly.tail(12)  # Last 12 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: Serialization Checkpoint\n",
    "\n",
    "We'll save our cleaned and enriched data in two formats:\n",
    "1. **JSON** - For API consumption and flexible schema\n",
    "2. **CSV** - For spreadsheet analysis and ML pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Serialize to JSON\n",
    "json_path = os.path.join(output_dir, 'cleaned_sales_data.json')\n",
    "\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(enriched_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"JSON Serialization:\")\n",
    "print(f\"  File: {json_path}\")\n",
    "print(f\"  Records: {len(enriched_data)}\")\n",
    "print(f\"  Size: {os.path.getsize(json_path):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize to CSV\n",
    "csv_path = os.path.join(output_dir, 'cleaned_sales_data.csv')\n",
    "\n",
    "enriched_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nCSV Serialization:\")\n",
    "print(f\"  File: {csv_path}\")\n",
    "print(f\"  Records: {len(enriched_df)}\")\n",
    "print(f\"  Columns: {len(enriched_df.columns)}\")\n",
    "print(f\"  Size: {os.path.getsize(csv_path):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify JSON can be loaded back\n",
    "with open(json_path, 'r') as f:\n",
    "    loaded_json = json.load(f)\n",
    "\n",
    "print(f\"\\nJSON Verification:\")\n",
    "print(f\"  Records loaded: {len(loaded_json)}\")\n",
    "print(f\"  Sample record keys: {list(loaded_json[0].keys())}\")\n",
    "\n",
    "# Verify CSV can be loaded back\n",
    "loaded_csv = pd.read_csv(csv_path)\n",
    "print(f\"\\nCSV Verification:\")\n",
    "print(f\"  Shape: {loaded_csv.shape}\")\n",
    "print(f\"  Columns match: {list(loaded_csv.columns) == list(enriched_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 12: Soft Interview Reflection\n",
    "\n",
    "### How Functions and Classes Have Helped in This Project\n",
    "\n",
    "The `SalesRecord` class was central to this data engineering pipeline. By encapsulating transaction data with methods like `clean()` and `total()`, I achieved **separation of concerns** — the cleaning logic lives with the data it operates on. This made the code more **maintainable**: when I needed to add new cleaning rules (like handling negative prices), I only modified one place.\n",
    "\n",
    "Functions like `profile_numeric_field()` and `aggregate_revenue_by_city()` promoted **reusability** — I could apply the same profiling logic to any numeric field without duplicating code. The **type hints** and **docstrings** made the code self-documenting, which would help any teammate understand the pipeline quickly.\n",
    "\n",
    "Most importantly, this structure supports **testing**: each method can be unit-tested in isolation, ensuring data quality at every step of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Dictionary\n",
    "\n",
    "This data dictionary merges field definitions from two sources:\n",
    "1. **Primary Source**: E-commerce sales CSV (`ecommerce_sales.csv`)\n",
    "2. **Secondary Source**: Coupon metadata JSON (`coupon_details.json`)\n",
    "\n",
    "## Original Fields (from Primary Source)\n",
    "\n",
    "| Field | Type | Description | Source |\n",
    "|-------|------|-------------|--------|\n",
    "| `order_id` | string | Unique identifier for each order (format: ORDnnnnnn) | Primary CSV |\n",
    "| `order_date` | string | Date of transaction in YYYY-MM-DD format | Primary CSV |\n",
    "| `customer_id` | string | Unique customer identifier (format: CUSTnnnnn) | Primary CSV |\n",
    "| `product` | string | Name of the product purchased | Primary CSV |\n",
    "| `category` | string | Product category (Electronics, Home & Kitchen, etc.) | Primary CSV |\n",
    "| `price` | float | Unit price in USD | Primary CSV |\n",
    "| `quantity` | integer | Number of units purchased | Primary CSV |\n",
    "| `coupon_code` | string | Promotional discount code applied (nullable) | Primary CSV |\n",
    "| `shipping_city` | string | Destination city for order delivery | Primary CSV |\n",
    "\n",
    "## Transformed Fields (computed from Primary)\n",
    "\n",
    "| Field | Type | Description | Source |\n",
    "|-------|------|-------------|--------|\n",
    "| `subtotal` | float | price × quantity before discounts | Computed: price × quantity |\n",
    "| `total_amount` | float | Final transaction amount after discount | Computed: subtotal × (1 - discount_rate) |\n",
    "| `year` | integer | Year extracted from order_date | Computed: from order_date |\n",
    "| `month` | integer | Month number (1-12) from order_date | Computed: from order_date |\n",
    "| `month_name` | string | Full month name (January, February, etc.) | Computed: from order_date |\n",
    "| `day_of_week` | string | Day name (Monday, Tuesday, etc.) | Computed: from order_date |\n",
    "| `quarter` | integer | Fiscal quarter (1-4) | Computed: from month |\n",
    "| `is_cleaned` | boolean | Flag indicating record passed through clean() | Computed: after cleaning |\n",
    "\n",
    "## Enriched Fields (from Secondary Source + Computation)\n",
    "\n",
    "| Field | Type | Description | Source |\n",
    "|-------|------|-------------|--------|\n",
    "| `discount_percent` | integer | Percentage discount from coupon (0-40) | Secondary JSON: mapped from coupon_code |\n",
    "| `discount_amount` | float | Dollar amount of discount applied | Computed: subtotal × discount_rate |\n",
    "\n",
    "## Engineered Features (derived for analysis)\n",
    "\n",
    "| Field | Type | Description | Source |\n",
    "|-------|------|-------------|--------|\n",
    "| `days_since_purchase` | integer | Days between order_date and reference date (2025-01-29) | Computed: date difference |\n",
    "| `is_high_value` | boolean | True if total_amount exceeds median | Computed: comparison to median |\n",
    "| `order_size_category` | string | Size bucket: 'Small' (qty≤1), 'Medium' (qty≤3), 'Large' (qty>3) | Computed: quantity bins |\n",
    "| `revenue_category` | string | Revenue bucket: 'Low' (<33%), 'Medium' (33-67%), 'High' (>67%) | Computed: percentile bins |\n",
    "\n",
    "## Secondary Source: Coupon Metadata Fields\n",
    "\n",
    "The coupon metadata (`coupon_details.json`) provided the discount mapping:\n",
    "\n",
    "| Coupon Code | Discount % | Description |\n",
    "|-------------|------------|-------------|\n",
    "| SAVE10 | 10 | Standard 10% discount |\n",
    "| SAVE15 | 15 | Weekend special |\n",
    "| SAVE20 | 20 | Newsletter subscriber discount |\n",
    "| SAVE25 | 25 | Seasonal sale |\n",
    "| WELCOME5 | 5 | New customer welcome |\n",
    "| FLASH30 | 30 | Flash sale limited time |\n",
    "| VIP40 | 40 | VIP member exclusive |\n",
    "| HOLIDAY15 | 15 | Holiday season special |\n",
    "| SUMMER10 | 10 | Summer clearance |\n",
    "| WINTER20 | 20 | Winter promotion |\n",
    "| FREESHIP | 0 | Free shipping (no price discount) |\n",
    "| LOYALTY25 | 25 | Loyalty program member |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Statistics\n",
    "\n",
    "Final overview of the processed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Records: {len(enriched_df)}\")\n",
    "print(f\"Total Columns: {len(enriched_df.columns)}\")\n",
    "print(f\"\\nOriginal Columns: 9\")\n",
    "print(f\"Transformed Columns: 7\")\n",
    "print(f\"Engineered Features: 4\")\n",
    "print(f\"\\nTotal Revenue: ${enriched_df['total_amount'].sum():,.2f}\")\n",
    "print(f\"Average Order Value: ${enriched_df['total_amount'].mean():.2f}\")\n",
    "print(f\"Total Discounts Given: ${enriched_df['discount_amount'].sum():,.2f}\")\n",
    "print(f\"\\nDate Range: {enriched_df['order_date'].min()} to {enriched_df['order_date'].max()}\")\n",
    "print(f\"Unique Customers: {enriched_df['customer_id'].nunique()}\")\n",
    "print(f\"Unique Cities: {enriched_df['shipping_city'].nunique()}\")\n",
    "print(f\"\\nData Quality: {sum(1 for r in all_records if r.is_cleaned)} records cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all columns in final dataset\n",
    "print(\"\\nALL COLUMNS IN FINAL DATASET:\")\n",
    "print(\"-\" * 40)\n",
    "for i, col in enumerate(enriched_df.columns, 1):\n",
    "    dtype = enriched_df[col].dtype\n",
    "    print(f\"{i:2}. {col:<25} ({dtype})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Lab Notebook**\n",
    "\n",
    "*Data Engineering Pipeline Complete*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
